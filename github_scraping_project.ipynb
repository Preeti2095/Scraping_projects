{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Github Scraping project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective of the project:-\n",
    "- To get the popular topics from github\n",
    "- Topic description and it's url\n",
    "- Then from each topic information of top repos are collected.\n",
    "- Like repo name, repo username, stars and respective urls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popular topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"topic.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repo Info that will be scraped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"repo_info.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's start the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's import required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### below code does the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get topic from github page\n",
    "def get_topic_title(doc):\n",
    "    topic_title_tags = doc.find_all(['p'], class_=\"f3 lh-condensed mb-0 mt-1 Link--primary\")\n",
    "    topic_title_tags = [topic.string for topic in topic_title_tags]\n",
    "    \n",
    "    return topic_title_tags\n",
    "\n",
    "#get description for each topic\n",
    "def get_topic_desc(doc):\n",
    "    topic_title_desc = doc.find_all(['p'], class_ = \"f5 color-fg-muted mb-0 mt-1\")\n",
    "    topic_title_desc = [topic.text.strip() for topic in topic_title_desc]\n",
    "    \n",
    "    return topic_title_desc\n",
    "\n",
    "#get url for each topic\n",
    "def get_topic_url(doc):\n",
    "    url_tags = doc.find_all(['p'], class_ = \"f5 color-fg-muted mb-0 mt-1\")\n",
    "    topic_url = []\n",
    "    for i in url_tags:\n",
    "        topic_url.append(\"https://github.com\"+i.parent[\"href\"])\n",
    "    return topic_url\n",
    "\n",
    "# this function scrape the topic\n",
    "def scrape_topic(url):\n",
    "#     url = \"https://github.com/topics\"\n",
    "    result = requests.get(url)\n",
    "    if result.status_code != 200:\n",
    "        raise Exception(\"Failes to load page {}\".format(url))\n",
    "    else:\n",
    "        html_doc = BeautifulSoup(result.text,\"html.parser\")\n",
    "\n",
    "        topic_info = {\n",
    "        \"title\":get_topic_title(html_doc),\n",
    "        \"Description\":get_topic_desc(html_doc),\n",
    "        \"URL\":get_topic_url(html_doc)\n",
    "        }\n",
    "\n",
    "        return pd.DataFrame(topic_info)\n",
    "    \n",
    "#convert no of stars into integers \n",
    "# Eg:- 69.7k to 69700\n",
    "def get_int(x):\n",
    "    if x[-1]==\"k\":\n",
    "        num = float(x[:-1]) * 1000\n",
    "        return int(num)\n",
    "    else:\n",
    "        return int(x)\n",
    "\n",
    "# this is the main function which scrape the repo info    \n",
    "def get_repo_info(h3_tag, star_tag):\n",
    "    a_tag = h3_tag.find_all(\"a\")\n",
    "    username = a_tag[0].text.strip()\n",
    "    reponame = a_tag[1].text.strip()\n",
    "    base_url = \"https://github.com\"\n",
    "    repo_url = base_url + (a_tag[1][\"href\"])\n",
    "    user_id_url = base_url + (a_tag[0][\"href\"])\n",
    "    star = star_tag.text.strip()\n",
    "    star = get_int(star)\n",
    "    \n",
    "    return username, reponame, repo_url, user_id_url, star\n",
    "\n",
    "def get_topic_repo(topic_url):\n",
    "    #sending request to topic page\n",
    "    request_page = requests.get(topic_url)\n",
    "    if request_page.status_code != 200:\n",
    "        raise Exception(\"Failes to load page {}\".format(topic_url))\n",
    "    else:\n",
    "        topic_page = BeautifulSoup(request_page.text, \"html.parser\")\n",
    "\n",
    "        #grab h3 tag\n",
    "        #grabing repo name and user name\n",
    "        h3_tag = topic_page.find_all([\"h3\"], class_ = \"f3 color-fg-muted text-normal lh-condensed\")\n",
    "\n",
    "        #grab span tag\n",
    "        #it contains star info\n",
    "        star_tag = topic_page.find_all([\"span\"], class_=\"Counter js-social-count\")\n",
    "\n",
    "\n",
    "        repo_dict = {\n",
    "        \"UserName\":[],\n",
    "        \"RepoName\":[],\n",
    "        \"RepoUrl\":[],\n",
    "        \"UserIDUrl\":[],\n",
    "        \"Star\":[]\n",
    "        }\n",
    "\n",
    "        UserName = []\n",
    "        RepoName = []\n",
    "        RepoUrl = []\n",
    "        UserIDUrl = []\n",
    "        Star = []\n",
    "\n",
    "        for (i,j) in zip(h3_tag,star_tag):\n",
    "            res = get_repo_info(i,j)\n",
    "            repo_dict[\"UserName\"].append(res[0])\n",
    "            repo_dict[\"RepoName\"].append(res[1])\n",
    "            repo_dict[\"RepoUrl\"].append(res[2])\n",
    "            repo_dict[\"UserIDUrl\"].append(res[3])\n",
    "            repo_dict[\"Star\"].append(res[4])\n",
    "\n",
    "    return pd.DataFrame(repo_dict)\n",
    "\n",
    "\n",
    "def scrap_github():\n",
    "    url = \"https://github.com/topics\"\n",
    "    df = scrape_topic(url)\n",
    "    \n",
    "    #creating the target directory\n",
    "    parent_dir = os.getcwd()\n",
    "    target_dir = \"scraped_data\"\n",
    "    full_path = os.path.join(parent_dir,target_dir)\n",
    "    if os.path.exists(full_path):\n",
    "        print(\"Target directory file already exists\")\n",
    "    else:\n",
    "        os.mkdir(full_path)\n",
    "    \n",
    "    #for each topic scraping github repo\n",
    "    for index, rows in df.iterrows():\n",
    "        title = rows['title']\n",
    "        url = rows[\"URL\"]\n",
    "        if os.path.exists(full_path + \"\\{}\".format(title) + \".csv\"):\n",
    "            print(\"Skipping as {} already exists\".format(title+\".csv\"))\n",
    "        else:\n",
    "            print(\"Scraping repo for topic '{}'\".format(title))\n",
    "            df = get_topic_repo(url)\n",
    "            df.to_csv(full_path + \"\\{}\".format(title) + \".csv\", index = None)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To see the result just call the function scrap_github()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target directory file already exists\n",
      "Skipping as 3D.csv already exists\n",
      "Skipping as Ajax.csv already exists\n",
      "Skipping as Algorithm.csv already exists\n",
      "Skipping as Amp.csv already exists\n",
      "Skipping as Android.csv already exists\n",
      "Skipping as Angular.csv already exists\n",
      "Skipping as Ansible.csv already exists\n",
      "Skipping as API.csv already exists\n",
      "Skipping as Arduino.csv already exists\n",
      "Skipping as ASP.NET.csv already exists\n",
      "Skipping as Atom.csv already exists\n",
      "Skipping as Awesome Lists.csv already exists\n",
      "Skipping as Amazon Web Services.csv already exists\n",
      "Skipping as Azure.csv already exists\n",
      "Skipping as Babel.csv already exists\n",
      "Skipping as Bash.csv already exists\n",
      "Skipping as Bitcoin.csv already exists\n",
      "Skipping as Bootstrap.csv already exists\n",
      "Skipping as Bot.csv already exists\n",
      "Skipping as C.csv already exists\n",
      "Skipping as Chrome.csv already exists\n",
      "Skipping as Chrome extension.csv already exists\n",
      "Skipping as Command line interface.csv already exists\n",
      "Skipping as Clojure.csv already exists\n",
      "Skipping as Code quality.csv already exists\n",
      "Skipping as Code review.csv already exists\n",
      "Skipping as Compiler.csv already exists\n",
      "Skipping as Continuous integration.csv already exists\n",
      "Skipping as COVID-19.csv already exists\n",
      "Skipping as C++.csv already exists\n"
     ]
    }
   ],
   "source": [
    "scrap_github()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that we are getting the desired result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
